{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic differentiation and jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import jax\n",
    "from jax import numpy as np\n",
    "from jax import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed():\n",
    "    return jax.random.PRNGKey(random.randint(0, 228))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_differentiation_methods(f, gradf, shape, repeat_times=5):\n",
    "    for i in range(repeat_times):\n",
    "        x = jax.random.uniform(seed(), shape=shape)\n",
    "        isclose = np.isclose(gradf(x), jax.grad(f)(x)).flatten()\n",
    "\n",
    "        print(f\"Iteration {i}: \", end='')\n",
    "        if np.all(isclose == True):\n",
    "            print(\"all components are close\")\n",
    "        else:\n",
    "            print(\"some components differ\")\n",
    "            print(f\"Max componentwise relative error is: {((gradf(x) - jax.grad(f)(x)) / gradf(x)).max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, y):\n",
    "    return np.exp(-(np.sin(x) - np.cos(y))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = jax.xla_computation(f)(np.ones(1337), np.ones(1337))\n",
    "with open(\"graph.dot\", \"w\") as file:\n",
    "    file.write(graph.as_hlo_dot_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1336.80s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    }
   ],
   "source": [
    "!dot graph.dot -Tpng > graph.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ f(A) = \\operatorname{tr}(e^A),\\, A \\in \\mathbb{R}^{n \\times n} $$\n",
    "\n",
    "$$ \\nabla f(A) = \\exp(A^{\\top}) $$ proved in task 4 in Matrix calculus hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(A):\n",
    "    return np.trace(sp.linalg.expm(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradf(A):\n",
    "    return sp.linalg.expm(A.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: some components differ\n",
      "Max componentwise relative error is: -6.508145452244207e-05\n",
      "Iteration 1: some components differ\n",
      "Max componentwise relative error is: -9.928335202857852e-06\n",
      "Iteration 2: some components differ\n",
      "Max componentwise relative error is: -4.052296208101325e-05\n",
      "Iteration 3: some components differ\n",
      "Max componentwise relative error is: -3.57690078089945e-05\n",
      "Iteration 4: some components differ\n",
      "Max componentwise relative error is: -1.8703463865676895e-05\n"
     ]
    }
   ],
   "source": [
    "compare_differentiation_methods(f, gradf, (20, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, different approaches give us different results with maximum relative error about $$10^{-5}$$ or less"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ f(x) = \\frac{1}{2} \\| x \\|^2,\\, x \\in \\mathbb{R}^n $$\n",
    "\n",
    "$$ f(x) = \\frac{1}{2} \\langle x,\\, x \\rangle $$\n",
    "\n",
    "$$ df(x) = \\langle x,\\, dx \\rangle $$\n",
    "\n",
    "$$ \\nabla f(x) = x $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L(x_0):\n",
    "    def wrapper(alpha):\n",
    "        nonlocal x_0\n",
    "        x = x_0\n",
    "        for i in range(10):\n",
    "            x = x - alpha[i] * x\n",
    "        return np.linalg.norm(x) / 2\n",
    "    \n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = float(jax.random.uniform(seed(), shape=(1,))[0])\n",
    "alpha_1 = jax.random.uniform(seed(), maxval=0.1, shape=(10,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.04536309, dtype=float32)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L(x_0)(alpha_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... :(\n",
    "    \n",
    "Gradient descent doesn't work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ f(x) = -\\log \\det X,\\, X \\in \\mathbb{R}^{n \\times n} $$\n",
    "\n",
    "$$ df(x) = -\\frac{\\det X \\cdot \\langle X^{-\\top},\\, dX \\rangle}{\\det X} = \\langle X^{-\\top},\\, dX \\rangle$$\n",
    "\n",
    "$$ \\nabla f(x) = -X^{-\\top} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(X):\n",
    "    return -np.log(np.linalg.det(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradf(X):\n",
    "    return -np.linalg.inv(X).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: some components differ\n",
      "Max componentwise relative error is: 0.00013778700667899102\n",
      "Iteration 1: some components differ\n",
      "Max componentwise relative error is: 9.875793330138549e-05\n",
      "Iteration 2: some components differ\n",
      "Max componentwise relative error is: 5.780803348898189e-06\n",
      "Iteration 3: some components differ\n",
      "Max componentwise relative error is: 1.0148980436497368e-05\n",
      "Iteration 4: some components differ\n",
      "Max componentwise relative error is: 0.00010453537106513977\n"
     ]
    }
   ],
   "source": [
    "compare_differentiation_methods(f, gradf, (20, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, different approaches give us different results with maximum relative error about $$10^{-4}$$ or less"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ f(x) = x^{\\top} x x^{\\top} x,\\, x \\in \\mathbb{R}^n $$\n",
    "\n",
    "$$ f(x) = \\langle x,\\, x \\rangle^2 $$\n",
    "\n",
    "$$ df(x) = 4 \\cdot \\langle x,\\, x \\rangle \\cdot \\langle x,\\, dx \\rangle = \\big\\langle 4 \\cdot \\langle x,\\, x \\rangle \\cdot x,\\, dx \\big\\rangle $$\n",
    "\n",
    "$$ \\nabla f(x) = 4 \\cdot \\langle x,\\, x \\rangle \\cdot x $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return (x.T @ x) * (x.T @ x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradf(x):\n",
    "    return 4 * (x.T @ x) * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: all components are close\n",
      "Iteration 1: all components are close\n",
      "Iteration 2: all components are close\n",
      "Iteration 3: all components are close\n",
      "Iteration 4: all components are close\n"
     ]
    }
   ],
   "source": [
    "compare_differentiation_methods(f, gradf, (100,))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
