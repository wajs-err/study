{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "from typing import Sequence\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Show on which platform JAX is running.\n",
    "print(\"JAX running on\", jax.devices()[0].platform.upper())\n",
    "\n",
    "# The learning rate for the optimizer:\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# Number of samples in each batch:\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "# Total number of epochs to train for:\n",
    "N_EPOCHS = 20\n",
    "\n",
    "# Population size for evolutionary algorithm\n",
    "POPULATION_SIZE = 10000\n",
    "\n",
    "(train_loader, test_loader), info = tfds.load(\n",
    "        'mnist', split=[\"train\", \"test\"], as_supervised=True, with_info=True\n",
    ")\n",
    "NUM_CLASSES = info.features[\"label\"].num_classes\n",
    "IMG_SIZE = info.features[\"image\"].shape\n",
    "\n",
    "train_loader_batched = train_loader.shuffle(\n",
    "        buffer_size=10_000, reshuffle_each_iteration=True\n",
    ").batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "test_loader_batched = test_loader.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"A simple multilayer perceptron model for image classification.\"\"\"\n",
    "    hidden_sizes: Sequence[int] = ()\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Flattens images in the batch.\n",
    "        x = x.reshape((x.shape[0], -1))\n",
    "        for size in self.hidden_sizes:\n",
    "                x = nn.Dense(features=size)(x)\n",
    "                x = nn.elu(x)\n",
    "        x = nn.Dense(features=NUM_CLASSES)(x)\n",
    "        return x\n",
    "\n",
    "@jax.jit\n",
    "def predict(params, inputs):\n",
    "    return net.apply({'params': params}, inputs)\n",
    "\n",
    "@jax.jit\n",
    "def loss_accuracy(params, data):\n",
    "    \"\"\"Computes loss and accuracy over a mini-batch.\n",
    "\n",
    "    Args:\n",
    "        params: parameters of the model.\n",
    "        bn_params: state of the model.\n",
    "        data: tuple of (inputs, labels).\n",
    "        is_training: if true, uses train mode, otherwise uses eval mode.\n",
    "\n",
    "    Returns:\n",
    "        loss: float\n",
    "    \"\"\"\n",
    "    inputs, labels = data\n",
    "    logits = predict(params, inputs)\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "            logits=logits, labels=labels\n",
    "    ).mean()\n",
    "    accuracy = jnp.mean(jnp.argmax(logits, axis=-1) == labels)\n",
    "    return loss, accuracy\n",
    "\n",
    "@jax.jit\n",
    "def update_model(state, grads):\n",
    "    return state.apply_gradients(grads=grads)\n",
    "\n",
    "def dataset_stats(params, data_loader):\n",
    "    \"\"\"Computes loss and accuracy over the dataset `data_loader`.\"\"\"\n",
    "    all_accuracy = []\n",
    "    all_loss = []\n",
    "    for batch in data_loader.as_numpy_iterator():\n",
    "        batch_loss, batch_aux = loss_accuracy(params, batch)\n",
    "        all_loss.append(batch_loss)\n",
    "        all_accuracy.append(batch_aux)\n",
    "    return {\"loss\": np.mean(all_loss), \"accuracy\": np.mean(all_accuracy)}\n",
    "\n",
    "net = MLP()\n",
    "\n",
    "solver_1 = optax.sgd(LEARNING_RATE, momentum = 0.9, nesterov = True)\n",
    "rng = jax.random.PRNGKey(0)\n",
    "dummy_data = jnp.ones((1,) + IMG_SIZE, dtype=jnp.float32)\n",
    "\n",
    "init_params = net.init({\"params\": rng}, dummy_data)[\"params\"]\n",
    "\n",
    "methods = [\"Simulated annealing\", \"SGD\"]\n",
    "metrics = dict.fromkeys(methods)\n",
    "\n",
    "for method in methods:\n",
    "        metrics[method] = dict.fromkeys([\"Train loss\", \"Test loss\",\n",
    "                                        \"Train accuracy\", \"Test accuracy\",\n",
    "                                        \"Time\"])\n",
    "\n",
    "        for key in metrics[method]:\n",
    "            metrics[method][key] = []\n",
    "\n",
    "def evaluate_model(params, metrics, method):\n",
    "    metr = dataset_stats(params, train_loader_batched)\n",
    "    metrics[method][\"Train loss\"].append(metr[\"loss\"])\n",
    "    metrics[method][\"Train accuracy\"].append(metr[\"accuracy\"])\n",
    "\n",
    "    metr = dataset_stats(params, test_loader_batched)\n",
    "    metrics[method][\"Test loss\"].append(metr[\"loss\"])\n",
    "    metrics[method][\"Test accuracy\"].append(metr[\"accuracy\"])\n",
    "\n",
    "    return metrics\n",
    "\n",
    "@jax.jit\n",
    "def grad_step(params, solver_state, batch):\n",
    "    # Performs a one step update.\n",
    "    (loss, aux), grad = jax.value_and_grad(loss_accuracy, has_aux=True)(\n",
    "            params, batch\n",
    "    )\n",
    "    updates, solver_state = solver_1.update(grad, solver_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, solver_state, loss, aux\n",
    "\n",
    "@jax.jit\n",
    "def add_noise(params, key, noise_scale=1e-3):\n",
    "    # Function to add Gaussian noise to parameters, handling a single key\n",
    "    return jax.tree_map(lambda p: p + noise_scale * jax.random.normal(key, p.shape, p.dtype), params)\n",
    "\n",
    "a = 0\n",
    "b = 0\n",
    "c = 0\n",
    "\n",
    "def grad_free_step(params, batch, temperature, noise_scale=1e-3):\n",
    "    noisy_params = add_noise(params, jax.random.split(jax.random.PRNGKey(int(time.time())), 1)[1], noise_scale)\n",
    "    loss, acc = loss_accuracy(noisy_params, batch)\n",
    "    prev_loss, prev_acc = loss_accuracy(params, batch)\n",
    "    # print(loss - prev_loss)\n",
    "    if loss <= prev_loss:\n",
    "        global a\n",
    "        a += 1\n",
    "        return noisy_params, loss, acc\n",
    "    elif jax.random.uniform(jax.random.split(jax.random.PRNGKey(int(time.time())), 1)[1]) <= jnp.exp(-(loss - prev_loss) / temperature):\n",
    "        global b\n",
    "        b += 1\n",
    "        return noisy_params, loss, acc\n",
    "    else:\n",
    "        global c\n",
    "        c += 1\n",
    "        return params, prev_loss, prev_acc\n",
    "\n",
    "for method in methods:\n",
    "    ### Evaluate model before training\n",
    "    params = init_params\n",
    "    metrics = evaluate_model(params, metrics, method)\n",
    "    metrics[method][\"Time\"].append(0)\n",
    "\n",
    "    if method == \"SGD\":\n",
    "        solver_state = solver_1.init(init_params)\n",
    "        start_time = time.time()\n",
    "        for epoch in range(N_EPOCHS):\n",
    "            train_accuracy_epoch = []\n",
    "            train_losses_epoch = []\n",
    "\n",
    "            for train_batch in train_loader_batched.as_numpy_iterator():\n",
    "                 params, solver_state, train_loss, train_acc = grad_step(\n",
    "                         params, solver_state, train_batch\n",
    "                 )\n",
    "                 metrics[method][\"Train loss\"].append(train_loss)\n",
    "                 metrics[method][\"Train accuracy\"].append(train_acc)\n",
    "        \n",
    "             metrics[method][\"Time\"].append(time.time() - start_time)\n",
    "             metrics = evaluate_model(params, metrics, method)\n",
    "    elif method == \"Simulated annealing\":\n",
    "        params = init_params\n",
    "        start_time = time.time()\n",
    "        temperature = 1.0\n",
    "        alpha = 0.99\n",
    "        while temperature > 1e-8:\n",
    "            noise = temperature * 7e-3\n",
    "\n",
    "            for train_batch in train_loader_batched.as_numpy_iterator():\n",
    "                params, train_loss, train_acc = grad_free_step(\n",
    "                        params, train_batch, temperature, noise_scale=noise\n",
    "                )\n",
    "                temperature *= alpha \n",
    "                metrics[method][\"Train loss\"].append(train_loss)\n",
    "                metrics[method][\"Train accuracy\"].append(train_acc)\n",
    "\n",
    "            metrics[method][\"Time\"].append(time.time() - start_time)\n",
    "            metrics = evaluate_model(params, metrics, method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Creating a mosaic layout\n",
    "fig, axs = plt.subplot_mosaic([['A', 'C'],\n",
    "                               ['B', 'D'],\n",
    "                               [\"E\", \"F\"]],\n",
    "                              figsize=(8, 9), constrained_layout=True)\n",
    "\n",
    "# Assuming 'methods' and 'metrics' are defined and contain the required data\n",
    "# Plotting the graphs in the specified layout with different markers for each method\n",
    "\n",
    "# Train Loss\n",
    "for method in methods:\n",
    "    axs['A'].semilogy(metrics[method][\"Train loss\"], label=method)\n",
    "axs['A'].set_title(\"Train Loss\")\n",
    "axs['A'].set_xlabel(\"Iteration\")\n",
    "axs['A'].set_ylabel(\"Cross-entropy\")\n",
    "axs['A'].legend()\n",
    "axs['A'].grid(linestyle=\":\")\n",
    "\n",
    "# Test Loss\n",
    "for method in methods:\n",
    "    marker = 'x' if method == \"SGD\" else 'o'  # 'x' for SGD, 'o' for ES\n",
    "    axs['B'].semilogy(metrics[method][\"Test loss\"], label=method, marker=marker)\n",
    "axs['B'].set_title(\"Test Loss\")\n",
    "axs['B'].set_xlabel(\"Epoch\")\n",
    "axs['B'].set_ylabel(\"Cross-entropy\")\n",
    "axs['B'].legend()\n",
    "axs['B'].grid(linestyle=\":\")\n",
    "\n",
    "# Train Accuracy\n",
    "for method in methods:\n",
    "    axs['C'].plot(metrics[method][\"Train accuracy\"], label=method)\n",
    "axs['C'].set_title(\"Train Accuracy\")\n",
    "axs['C'].set_xlabel(\"Iteration\")\n",
    "axs['C'].set_ylabel(\"Accuracy\")\n",
    "axs['C'].legend()\n",
    "axs['C'].grid(linestyle=\":\")\n",
    "\n",
    "# Test Accuracy\n",
    "for method in methods:\n",
    "    marker = 'x' if method == \"SGD\" else 'o'  # 'x' for SGD, 'o' for ES\n",
    "    axs['D'].plot(metrics[method][\"Test accuracy\"], label=method, marker=marker)\n",
    "axs['D'].set_title(\"Test Accuracy\")\n",
    "axs['D'].set_xlabel(\"Epoch\")\n",
    "axs['D'].set_ylabel(\"Accuracy\")\n",
    "axs['D'].legend()\n",
    "axs['D'].grid(linestyle=\":\")\n",
    "\n",
    "# Train Epoch from Time\n",
    "for method in methods:\n",
    "    marker = 'x' if method == \"SGD\" else 'o'  # 'x' for SGD, 'o' for ES\n",
    "    axs['E'].plot(metrics[method][\"Time\"], metrics[method][\"Test loss\"], label=method, marker=marker)\n",
    "axs['E'].set_title(\"Test loss\")\n",
    "axs['E'].set_xlabel(\"Time (s)\")\n",
    "axs['E'].set_ylabel(\"Cross-entropy\")\n",
    "axs['E'].legend()\n",
    "axs['E'].grid(linestyle=\":\")\n",
    "\n",
    "# Test Accuracy from Time\n",
    "for method in methods:\n",
    "    marker = 'x' if method == \"SGD\" else 'o'  # 'x' for SGD, 'o' for ES\n",
    "    axs['F'].plot(metrics[method][\"Time\"], metrics[method][\"Test accuracy\"], label=method, marker=marker)\n",
    "axs['F'].set_title(\"Test Accuracy\")\n",
    "axs['F'].set_xlabel(\"Time (s)\")\n",
    "axs['F'].set_ylabel(\"Accuracy\")\n",
    "axs['F'].legend()\n",
    "axs['F'].grid(linestyle=\":\")\n",
    "\n",
    "plt.suptitle(f\"Training a Neural Network on MNIST.\\n Simulated annealing.\\n{sum(jnp.prod(jnp.array(param.shape)) for param in jax.tree_util.tree_leaves(params))} trainable parameters.\")\n",
    "\n",
    "plt.savefig(\"SA_vs_SGD.svg\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
